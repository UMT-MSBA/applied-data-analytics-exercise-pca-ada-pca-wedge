{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2d16e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd0f38e",
   "metadata": {},
   "source": [
    "### Reading in the data\n",
    "\n",
    "For this analysis we have our data already prepared. `prod_sales` will be a data frame with 1001 columns and 2674 rows. Each row is an owner and the owner number is in the first column. The remaining 1000 columns are the purchases by that owner across the top 1000 products. Let's read the data in and look at the total sales for the top 20 products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf490dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_sales = pd.read_csv(\"owner_level_top_prod_sales.txt\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98c150b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4539a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_summary = prod_sales.iloc[:,1:].sum().reset_index()\n",
    "product_summary = product_summary.rename(columns={\"index\":\"product\",0:\"total_sales\"})\n",
    "product_summary = product_summary.set_index(\"product\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17460a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_summary.sort_values(by=\"total_sales\").tail(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868f3332",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_summary.sort_values(by='total_sales').tail(n=10).plot(kind=\"bar\")\n",
    "plt.title(\"Total Sales by Product\")\n",
    "plt.ylabel(\"Sales across All Owners\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7973a7b6",
   "metadata": {},
   "source": [
    "### PCA on Wedge product sales\n",
    "\n",
    "These data represent a typical use case for PCA, with our 1000 dimensions of spend data for each owner. Let's fit a PCA model on the product column. We'll use the PCA function from sci-kit learn. You can learn much more about the functionality by reading the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135ba113",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299f934c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(prod_sales.drop(columns=\"owner\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47670e9b",
   "metadata": {},
   "source": [
    "Typically the first thing we do when working with PCA output is to look at the explained variance. Our PCA object has two variance statistics: `explained_variance_` and `explained_variance_ratio_`. The former is the variance explained by the component in raw terms, the latter normalizes by the total variance of the data set. Let's look at the normalized value for the first 20 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaf4da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = pd.DataFrame(pca.explained_variance_ratio_)\n",
    "#showing the first value\n",
    "ax = explained_variance.head(20).plot.bar(legend=False, figsize=(8, 4))\n",
    "ax.set_xlabel('Component')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8f691a",
   "metadata": {},
   "source": [
    "We can see that the first component explains a much larger portion of the variance than any other component. The next two explain more than the fourth. After the fifth component we see a general flattening of the curve, at least at this scale. \n",
    "\n",
    "It can also be nice to look at cumulative variance over a larger range of components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bd06d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cume_explained_variance = pd.DataFrame(pca.explained_variance_ratio_).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3ed1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing the first value\n",
    "ax = cume_explained_variance.head(150).plot.line(legend=False, figsize=(8, 4))\n",
    "ax.set_xlabel('Component')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7670f5",
   "metadata": {},
   "source": [
    "As we can see, you need around 150 components to get to 90% variance explained. There's nothing magic about 90%, although it's often a convenient cutoff when we have fewer initial dimensions. It would be much too burdensome to try to interpret all of these components. \n",
    "\n",
    "### Investigating loadings\n",
    "\n",
    "In order to understand a PCA model, we typically look at the first $N$ components to try to understand which aspects of the data are being used to explain variation. The loadings on the components allow us to do this, but looking at how the sales of each product influence the linear combination represented by each component. Remember: values that are large in absolute value play the greatest role. Additionally, values of different signs indicate the spectrum of the component. Typically we'll see both positive and negative values, indicating a compenent is contrasting between two different types of observations. \n",
    "\n",
    "This plotting code below is found from [this helpful blog post](https://medium.com/analytics-vidhya/pca-and-how-to-interpret-it-with-python-8aa664f7a69a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246f9c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = pd.DataFrame(pca.components_, columns=prod_sales.columns[1:])\n",
    "loadings.transpose().sort_values(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b0f082",
   "metadata": {},
   "source": [
    "This display isn't the most helpful, but here we're seeing how each product loads on each component. We've sorted by the first component (number zero). We can see that the smallest loading is positive but close to zero. The largest loading, chicken breasts, has a weight of 0.25. Let's take a closer look at this component, plotting the 20 largest loadings.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d244559",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_loading = loadings.loc[0, :].sort_values(0).tail(n=20)\n",
    "pc_loading.plot.bar()\n",
    "plt.ylabel(\"PC1 Loading\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabb0634",
   "metadata": {},
   "source": [
    "This component has positive loadings for every product and has the highest weights on the most popular product. Essentially, this component measures how much the owner spends at the Wedge. \n",
    "\n",
    "Let's take a look at the second component. Before we do that, let's write a function that will plot the component for us. This was developed from the blog post mentioned above and took a lot of trial and error to get right.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4121f4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_component(component,cutoff=0.05) : \n",
    "    \n",
    "    pc_loading = loadings.transpose().iloc[:,component-1]\n",
    "    pc_loading = pc_loading.loc[pc_loading.abs() > cutoff].sort_values()\n",
    "    max_pc = max(pc_loading.abs())\n",
    "    colors = ['C0' if l > 0 else 'C1' for l in pc_loading]\n",
    "\n",
    "    pc_loading.plot.bar(color = colors)\n",
    "    ax = plt.gca()\n",
    "    plt.axhline(y=0,c=\"#888888\")\n",
    "    ax.set_ylim(-max_pc, max_pc)\n",
    "    plt.tight_layout()\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4815db",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_component(2,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb6a8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_component(3,0.06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2408a206",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_component(4,0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f8a73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_component(5,0.06)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e312aa7c",
   "metadata": {},
   "source": [
    "In class we'll spend some time discussing these loadings. The R example (which is the solution to the old PCA assignment) dives deeper into these. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
